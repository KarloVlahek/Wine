---
title: 'Wine'
author: "Karlo Vlahek"
date: "4/29/2022"
output:
  md_document:
    variant: markdown_github
always_allow_html: true
---
```{r, include=FALSE}
library(tidyverse)
library(LICORS)
library(foreach)
library(knitr)
library(kableExtra)
library(ggplot2)
library(ggpubr)
library(factoextra)
library(ggcorrplot)
library(ggfortify)
library(patchwork)
wine = read.csv("/Users/karlo/Desktop/Graduate_School/Spring_2022/Data_Mining/wine.csv")
set.seed(1234)

```

We first begin with a clustering algorithm which will partition the various types of wine into their mutually exclusive groupings. Centering and scaling the data allows for easier interpretability down the line. This is also done to represent a solid measure of distance. For example, the fixed acidity feature has a variation that is larger by a factor of 80 from that of the citric acid feature. If we do not center and scale the data, distance would be biased towards and driven more by the former feature as opposed to the latter. Thus standardizing gives the features equal weight. The choice of K is intuitive. Since there are two 'classes' of wine, we will implement 2 clusters in the algorithm. The other K we will choose is 7 as there are 7 different levels of quality.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# Center and Scale
X = wine[, -(12:13)]
X = scale(X,center=TRUE, scale = TRUE)

# Extracting scales and centers for later
mu = attr(X,"scaled:center")
sig = attr(X,"scaled:scale")

# Run k-means and k-means++ with 2 clusters because there are red and white wines
clust_color = kmeans(X,2, nstart = 25)
clust_color_pp = kmeanspp(X,2, nstart = 25)

## Run k-means and k-means++ with 7 clusters because there are 7 wine qualities
clust_quality = kmeans(X,7, nstart = 25)
clust_quality_pp = kmeanspp(X,7, nstart = 25)

# Taking away the normalization components, we can interpret the centroids in their raw form
clust_color_pp$center[1,]*sig + mu
clust_color_pp$center[2,]*sig + mu
clust_quality_pp$center[2,]*sig + mu
clust_quality_pp$center[4,]*sig + mu
clust_quality_pp$center[7,]*sig + mu

# Which wine colors are in which cluster? 
color_cluster1 = as.data.frame(wine[which(clust_color$cluster==1),]$color)
color_cluster2 = as.data.frame(wine[which(clust_color$cluster==2),]$color)

# Which wine level qualities are in which cluster?
score_cluster7 = as.data.frame(wine[which(clust_quality_pp$cluster==7),]$quality)
score_cluster5 = as.data.frame(wine[which(clust_quality_pp$cluster==5),]$quality)
score_cluster3 = as.data.frame(wine[which(clust_quality_pp$cluster==3),]$quality)
```

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
zscore_features1 = knitr::kable(clust_color_pp$center, col.names = c("Fixed Acidity", "Volatile Acidity", "Citric Acid", "Residual Sugar", "Chlorides", "Free Sulfur Dioxide", "Total Sulfur Dioxide","Density", "pH", "Sulphates", "Alcohol"),
                               caption = "Z-Score of Features for Color Cluster",
                               booktabs = TRUE) %>% 
  kable_styling(position = "center")
                               
zscore_features1

zscore_features2 = knitr::kable(clust_quality$center, col.names = c("Fixed Acidity", "Volatile Acidity", "Citric Acid", "Residual Sugar", "Chlorides", "Free Sulfur Dioxide", "Total Sulfur Dioxide","Density", "pH", "Sulphates", "Alcohol"),
                               caption = "Z-Score of Features for Quality Cluster",
                               booktabs = TRUE) %>% 
  kable_styling(position = "center")
                               
zscore_features2

clust_color_pp$tot.withinss # Total Within Sum of Squares for K-Means++ Wine Color Clustering
clust_quality_pp$tot.withinss # Total Within Sum of Squares for K-Means++ Quality Level Clustering
clust_color_pp$betweenss # Total Between Sum of Squares for K-Means++ Wine Color Clustering
clust_quality_pp$betweenss # Total Between Sum of Squares for K-Means++ Quality Level Clustering
```

As we can see above, these are the center points (or centroids) for both cluster types across all 11 features used, z-scored. Thus, the interpretation of theses numbers are as a z-score. That is, how many standard deviations above the mean of the entire data set a cluster is for a given feature. For example, the residual sugar amount for the cluster by wine color is .1998 standard deviations above the mean of all data points. Extending the interpretation, we can see that cluster one has below average fixed acidity, volatile acidity, chlorides, density, pH, and sulphates. Similar interpretations can be made for the quality cluster (i.e. - the quality score by the wine snobs) and above average features. When clustering for wine color, the within sum of squares and between sum of squares are the same. However, when clustering for quality, the K-Means++ algorithm returns a higher within sum of squares and lower between sum of squares. Thus, we will use K-Means++ for both the wine color and quality plots to determine the actual vs. predicted groupings.We will visualize the various relationships of various features and how well K-Means++ has clustered the labels.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# Color of Wine Clustering

# Comparing Acidity and how it relates to color of wine
a1=qplot(fixed.acidity, volatile.acidity, data = wine, color = factor(clust_color_pp$cluster), alpha = 0.8)
a2=qplot(fixed.acidity, volatile.acidity, data = wine, color = color, alpha = 0.8)

a1+a2

# Comparing Citric Acid levels and pH balance and how it relates to color of wine
b1=qplot(citric.acid, pH, data = wine, color = factor(clust_color_pp$cluster), alpha = 0.8)
b2=qplot(citric.acid, pH, data = wine, color = color, alpha = 0.8)

b1+b2

# Comparing Free and Total Sulfur Dioxide and how it relates to color of wine
c1=qplot(free.sulfur.dioxide, total.sulfur.dioxide, data = wine, color = factor(clust_color_pp$cluster), alpha = 0.8)
c2=qplot(free.sulfur.dioxide, total.sulfur.dioxide, data = wine, color = color, alpha = 0.8)

c1+c2
# Comparing Alcohol and Sulphates and how it relates to color of wine
d1=qplot(alcohol, sulphates, data = wine, color = factor(clust_color_pp$cluster), alpha = 0.8)
d2=qplot(alcohol, sulphates, data = wine, color = color, alpha = 0.8)

d1+d2

#ggarrange(a1,a2,b1,b2,c1,c2,d1,d2,
          #labels = c("Acidities Clustered","Acidities by Color", "Citric Acid v. pH Clustered", "Citric Acid v. pH by Color","Free Sulfur v. Total Sulfur Clustered","Free Sulfur v. Total Sulfur by Color", "Alcohol v. Sulphates Clustered", "Alcohol v. Sulphates by Color"),
         # ncol = 2, nrow = 4)
```

Above, we observe various feature relationships in two dimensions. Unfortunately, observing all 11 features in 11 dimensions would be quite ambiguous and tedious. We see here that the interpretation of K-Means++ clustering relative to the actual wine color groupings are rather on par. The nice component of this analysis is the interpretability. There seems to be a relatively clear distinction between red and white wines, and the features represented here strongly indicate association

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
# Quality Level Clustering

# Comparing Acidity and how it relates to wine quality
a11=qplot(fixed.acidity, volatile.acidity, data = wine, color = factor(clust_quality_pp$cluster), alpha = 0.8)
a22=qplot(fixed.acidity, volatile.acidity, data = wine, color = factor(quality), alpha = 0.8)

a11+a22
# Comparing Citric Acid levels and pH balance and how it relates to wine quality
b11=qplot(citric.acid, pH, data = wine, color = factor(clust_quality_pp$cluster), alpha = 0.8)
b22=qplot(citric.acid, pH, data = wine, color = factor(quality), alpha = 0.8)
b11+b22
# Comparing Free and Total Sulfur Dioxide and how it relates to wine quality
c11=qplot(free.sulfur.dioxide, total.sulfur.dioxide, data = wine, color = factor(clust_quality_pp$cluster), alpha = 0.8)
c22=qplot(free.sulfur.dioxide, total.sulfur.dioxide, data = wine, color = factor(quality), alpha = 0.8)
c11+c22
# Comparing Alcohol and Sulphates and how it relates to wine quality
d11=qplot(alcohol, sulphates, data = wine, color = factor(clust_quality_pp$cluster), alpha = 0.8)
d22=qplot(alcohol, sulphates, data = wine, color = factor(quality), alpha = 0.8)

d11+d22
#ggarrange(a11,a22,b11,b22,c11,c22,d11,d22,
         # labels = c("Acidities Clustered","Acidities by Quality", "Citric Acid v. pH Clustered", "Citric Acid v. pH by Quality","Free Sulfur v. Total Sulfur Clustered","Free #Sulfur v. Total Sulfur by Quality", "Alcohol v. Sulphates Clustered", "Alcohol v. Sulphates by Quality"),
          #ncol = 2, nrow = 4)
```

Observing the same relationships but for wine quality, we see the interpretations are substantially more ambiguous and perhaps even difficult to distinguish. Thus, wine quality is ambiguous. This makes sense relative to how easily K-Means++ clustering grouped reds and whites. A logical insinuation could be that color has a strong association with the chemical process grapes undergo to evidently make wine. Quality, however, was measured by wine tasting - a subjective feature. Judges rating the quality of the wine certainly have personal preferences. Thus, when attempting to cluster by quality level and observing the various feature relationships, it is difficult to distinguish what relationships among the features imply something about the quality.


Next, we move onto a Principle Component Analysis (PCA) to determine if this methodology allows for an easier interpretation of labels, and if this emerges naturally from applying this technique.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
ggcorrplot::ggcorrplot(cor(X), hc.order =TRUE)
```

When observing correlations of features, we re-order the features according to hierarchical clustering. In the top left and bottom right regions of the hierarchical correlation plot, we can observe strong, negative correlations. Unless you are a wine maker, a chemist, or a wine connoisseur, perhaps these correlations do not make a lot of sense. Let's take an example of the negative correlation between alcohol level and sulfur dioxide. A relatively straightforward example from this plot could be the negative correlation between density and alcohol. Ethanol is less dense than water, and the higher the concentration, the lower the density. This is because the molecules of ethanol are not as densely packed as, say, water. Chemical relationships are vast, so it is important to disclaim that correlation may not imply causation in this case. For this reason, it actually may not be best practice to consolidate these chemical components of wine just because they appear to be similar.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
PCAwine=prcomp(X, scale=TRUE, rank =3)
summary(PCAwine)
```

There are several takeaways from observing the summary of the analysis. We see the standard deviation of the PCs is highest in PC1. The proportion of variance each PC accounts for from the original data is somewhat the same, but this proportion is highest in PC1.Cumulatively, all three principle components account for a little more than two-thirds of the variation from the original data.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
knitr::kable(round(PCAwine$rotation[,1:3],2),
             col.names = c("PC1", "PC2","PC3"),
                               caption = "Principle Components",
                               booktabs = TRUE)
```

Looking at the table, we notice similar loadings in PC1 as the correlation plot. The higher, positive magnitudes associated with each feature represents stronger positive correlations and higher, negative magnitudes associated with each feature represents stronger, negative correlations. PC2 may be a bit more ambiguous, in that all features but pH and alcohol are positive. PC3 is similar but still different from PC1.

```{r, echo=FALSE, error=FALSE, message=FALSE, warning =FALSE}
autoplot(PCAwine, data = wine, colour = 'color', alpha=0.8)
wine$quality = wine$quality %>% as.factor
autoplot(PCAwine, data = wine, colour = 'quality', alpha = 0.8, 
         loadings = TRUE, loadings.colour = 'pink',
         loadings.label = TRUE, loadings.label.size = 3)
```

As the clustering methods showed, the color of the wine is easily separated by principle components and they are easily distinguished. We can also make the same conclusion about the quality level of wine (i.e. - it is ambiguous). By consolidating the features for each data point, the principle component does a great job at associating the linear summaries for color of wine.


Both clustering and principle components analysis come to the same conclusions more or less. However, K-Means++ offers slightly easier interpretability since we are able to observe relationships while holding clusters constant. The major difference between the clustering methods and the PCA analyses are that the former consolidates by rows (observations) depending on how you choose the number of clusters (k) while the latter consolidates the columns (features). In doing so, there are multiple things to say. Clustering is mutually exclusive meaning each point is a member of only one cluster. A PCA, however, assumes that a data point is some combination of many features which is linearly summarized by projecting each data point and attempting to preserve variation of the data. As alluded to before hand, this may simplify something that may better be interpreted in its original form. Thus, both methodologies yield similar conclusions. However,in this instance, K-Means++ allows for a richer interpretation of the data in a multitude of ways.











